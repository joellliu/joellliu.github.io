<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiang Liu</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jiang Liu</name>
              </p>
              <p>I am a PhD student at <a href="https://engineering.jhu.edu/ece/">Department of Electrical and Computer Engineering</a>, <a href="https://www.jhu.edu/">Johns Hopkins University</a> (JHU),
                advised by <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Prof. Rama Chellappa</a>.
                I am a member of <a href="https://aiem.jhu.edu">AIEM lab </a>, and  <a href=https://www.cis.jhu.edu/> CIS</a>.
                I received my MSE degree from JHU in 2022, and BSE degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2019 advised by <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Prof. Jianjiang Feng</a>
                and <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>.


              </p>
              <p>
                In summer 2022, I worked as an Applied Scientis Intern at Amazon AWS AI working on vision-language problem mentored by
                <a href="huiding.org">Dr. Hui Ding</a>,
                <a href="https://zhaoweicai.github.io/">Dr. Zhaowei Cai</a>, and
                <a href="https://www.ytzhang.net/">Dr. Yuting Zhang</a>. I've also worked as a Deep Learning Research Scientist Intern
                at <a href="https://subtlemedical.com/">Subtle Medical</a> developing novel Transformer-based magnetic resonance imaging (MRI) algorithms.

              </p>

              <p style="text-align:center">
                <a href="mailto:jiangliu@jhu.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IbeXR9cAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/joellliu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiangliu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests focus on building trustworthy AI systems that can benefit human beings.
                A major part of my research focuses on developing principled algorithms for defending AI systems against
                adversarial attacks. Besides adversarial robustness,
                I'm also interested in developing computer vision and machine learning techniques to improve healthcare, such as medical imaging.
              </p>
            </td>
          </tr>
        </tbody></table>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                 <tr onmouseout="poly_stop()" onmouseover="poly_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="poly_image" style="opacity: 1;">
                <img src="./images/poly_1.png" width="160"></div>
              <img src="./images/poly_2.png" width="160">
            </div>
            <script type="text/javascript">
              function sac_start() {
                document.getElementById('poly_image').style.opacity = "0";
              }

              function sac_stop() {
                document.getElementById('poly_image').style.opacity = "1";
              }
              sac_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2302.07387">
              <papertitle> PolyFormer: Referring Image Segmentation as Sequential Polygon Generation
              </papertitle>
            </a>
            <br>
            <strong>Jiang Liu*</strong>, Hui Ding*, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, R. Manmatha (*equal contribution)
            <br>
            <em>CVPR</em>, 2023
            <br>
            <a href="https://joellliu.github.io/polyformer">Project Page</a> /
            <a href="https://arxiv.org/abs/2302.07387">arXiv</a> /
            <a href="bibs/Liu_2023_CVPR.bib">bibtex</a> /
            <!-- <a href="https://github.com/hsouri/Sleeper-Agent">code</a> -->
<!--             <a href="https://github.com/hsouri/hsouri.github.io/blob/master/Data/khorramshahi2020gans.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              In this work, instead of directly predicting the pixel-level segmentation masks, the problem of referring
              image segmentation is formulated as sequential polygon generation, and the predicted polygons can be later
              converted into segmentation masks. This is enabled by a new sequence-to-sequence framework, Polygon
              Transformer (PolyFormer), which takes a sequence of image patches and text query tokens as input,
              and outputs a sequence of polygon vertices autoregressively. </p>
          </td>
        </tr>
         <tr onmouseout="sac_stop()" onmouseover="sac_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="sac_image" style="opacity: 1;">
                <img src="./images/sac_1.png" width="160"></div>
              <img src="./images/sac_2.png" width="160">
            </div>
            <script type="text/javascript">
              function sac_start() {
                document.getElementById('sac_image').style.opacity = "0";
              }

              function sac_stop() {
                document.getElementById('sac_image').style.opacity = "1";
              }
              sac_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Segment_and_Complete_Defending_Object_Detectors_Against_Adversarial_Patch_Attacks_CVPR_2022_paper.html">
              <papertitle> Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection
              </papertitle>
            </a>
            <br>
            <strong>Jiang Liu</strong>,
            Alexander Levine, Chun Pong Lau, Rama Chellappa, Soheil Feizi
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Segment_and_Complete_Defending_Object_Detectors_Against_Adversarial_Patch_Attacks_CVPR_2022_paper.pdf">PDF</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Liu_Segment_and_Complete_CVPR_2022_supplemental.pdf">Supp</a> /
            <a href="https://arxiv.org/abs/2112.04532">arXiv</a> /
            <a href="bibs/Liu_2022_CVPR.bib">bibtex</a> /
            <a href="https://github.com/joellliu/SegmentAndComplete">code</a> /
            <a href="https://aiem.jhu.edu/datasets/apricot-mask">Apricot-Mask Dataset</a>
            <!-- <a href="https://github.com/hsouri/Sleeper-Agent">code</a> -->
<!--             <a href="https://github.com/hsouri/hsouri.github.io/blob/master/Data/khorramshahi2020gans.bib">bibtex</a> -->
            <br>
            <p></p>
            <p>
              In this paper, we propose Segment and Complete defense (SAC),
              a general framework for defending object detectors against patch attacks
              through detection and removal of adversarial patches.
              Our experiments on COCO and xView datasets demonstrate that SAC achieves superior robustness even
              under strong adaptive attacks with no reduction in performance on clean images, and generalizes well to
              unseen patch shapes, attack budgets, and unseen attack methods. Furthermore, we present the APRICOT-Mask
              dataset, which augments the APRICOT dataset with pixel-level annotations of adversarial patches. </p>
          </td>
        </tr>
        <tr onmouseout="mat_stop()" onmouseover="mat_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="mat_image" style="opacity: 1;">
                <img src="./images/MAT.png" width="160"></div>
              <img src="./images/MAT.png" width="160">
            </div>
            <script type="text/javascript">
              function mat_start() {
                document.getElementById('mat_image').style.opacity = "0";
              }

              function mat_stop() {
                document.getElementById('mat_image').style.opacity = "1";
              }
              mat_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/9798870">
              <papertitle> Mutual Adversarial Training: Learning together is better than going alone
              </papertitle>
            </a>
            <br>
            <strong>Jiang Liu</strong>,
            Chun Pong Lau,
            Hossein Souri,
            Soheil Feizi,
            Rama Chellappa
            <br>
            <em>IEEE Transactions on Information Forensics and Security (TIFS)</em>, 2022
            <br>
            <a href="https://arxiv.org/pdf/2112.05005.pdf">PDF</a> /
            <a href="https://arxiv.org/abs/2112.05005">arXiv</a> /
            <a href="bibs/Liu_2022_mutual.bib">bibtex</a>
            <br>
            <p></p>
            <p>
              In this paper, we propose mutual adversarial training (MAT), in which multiple models are trained
              together and share the knowledge of adversarial examples to achieve improved robustness.
              MAT allows robust models to explore a larger space of adversarial samples, and
              find more robust feature spaces and decision boundaries. We show that MAT can improve model robustness for
              both <strong>single</strong> and <strong>multiple</strong> perturbations. </p>
          </td>
        </tr>

         <tr onmouseout="mmt_stop()" onmouseover="mmt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="mmt_image" style="opacity: 1;">
                <img src="./images/mmt_2.png" width="160"></div>
              <img src="./images/mmt_1.png" width="160">
            </div>
            <script type="text/javascript">
              function mmt_start() {
                document.getElementById('mmt_image').style.opacity = "0";
              }

              function mmt_stop() {
                document.getElementById('mmt_image').style.opacity = "1";
              }
              mmt_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2204.13738">
              <papertitle> One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation
              </papertitle>
            </a>
            <br>
            <strong>Jiang Liu</strong>,
            Srivathsa Pasumarthi, Ben Duffy, Enhao Gong, Greg Zaharchuk, Keshav Datta
            <br>
            <em>Arxiv</em>, 2022
            <br>
            <a href="https://arxiv.org/pdf/2204.13738.pdf">PDF</a> /
            <a href="https://arxiv.org/abs/2204.13738">arXiv</a> /
            <a href="bibs/Liu_2022_one.bib">bibtex</a>
            <br>
            <p></p>
            <p>
              In this paper, we formulate missing data imputation as a sequence-to-sequence learning problem and
              propose a multi-contrast multi-scale Transformer (MMT), which can take <em>any</em> subset of input contrasts and
              synthesize those that are missing. Thanks to the proposed multi-contrast Swin Transformer blocks,
              it can efficiently capture intra- and inter-contrast dependencies for accurate image synthesis.
              Moreover, MMT is inherently <strong>interpretable</strong> . It allows us to understand the importance of each
              input contrast in different regions by analyzing the in-built attention maps of Transformer blocks in the decoder.
          </td>
        </tr>

        <tr onmouseout="ijsat_stop()" onmouseover="ijsat_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id="ijsat_image" style="opacity: 1;">
                <img src="./images/ijsat_1.png" width="160"></div>
              <img src="./images/ijsat_2.png" width="160">
            </div>
            <script type="text/javascript">
              function ijsat_start() {
                document.getElementById('ijsat_image').style.opacity = "0";
              }

              function ijsat_stop() {
                document.getElementById('ijsat_image').style.opacity = "1";
              }
              ijsat_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2112.06323">
              <papertitle> Interpolated Joint Space Adversarial Training for Robust and Generalizable Defenses
              </papertitle>
            </a>
            <br>
            Chun Pong Lau,
            <strong>Jiang Liu</strong>,
            Hossein Souri, Wei-An Lin, Soheil Feizi, Rama Chellappa
            <br>
            <em>Arxiv</em>, 2021
            <br>
            <a href="https://arxiv.org/pdf/2112.06323.pdf">PDF</a> /
            <a href="https://arxiv.org/abs/2112.06323">arXiv</a> /
            <a href="bibs/lau_2021_ijsat.bib">bibtex</a>
            <br>
            <p></p>
            <p>
              We propose a novel threat model called Joint Space Threat Model (JSTM), which exploit the underlying manifold information with Normalizing Flow,
              ensuring that exact manifold assumption holds. Under JSTM, we develop novel adversarial attacks and defenses. Furthermore,
              we propose the Robust Mixup strategy in which we maximize the adversity of the interpolated images and gain robustness and prevent
              overfitting.
          </td>
        </tr>


						



					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">

                Source code credit to  <a href="https://jonbarron.info/">Dr. Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
